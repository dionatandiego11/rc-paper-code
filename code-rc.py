# -*- coding: utf-8 -*-
"""Alfa_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6x-b6HEGH2Vo2V2FZmXarAeDRRETnoX
"""

! pip install nolds

from scipy import sparse
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
from mpl_toolkits.mplot3d import Axes3D

# Reservoir Computing
# A reservoir computing é um método simples para treinar redes neurais recorrentes sem retropropagação ao longo do tempo
# e os notórios problemas associados de gradiente de fuga e explosão. Os passos básicos são:
# - Inicialize aleatoriamente pesos de rede neural recorrentes.
# - Corrija os pesos de conexão ocultos.
# - Treine a camada de saída linear com regressão linear.

m0 = -8/7
m1 = -5/7
alpha = 13
beta = 19
h = lambda x: m1 * x + ( (m0 - m1) * (abs(x + 1) - abs(x - 1)) / 2)

def derivative(t, v, d):
    dxdt = alpha * (v[1] - v[0] - h(v[0]))
    dydt = v[0] - v[1] + v[2]
    dzdt = - beta * v[1]
    return [dxdt, dydt, dzdt]

from scipy.integrate import ode
v0, t0 = [0.7, 0.0, 0.0], 0.0

r = ode(derivative).set_integrator('dopri5')
r.set_initial_value(v0, t0).set_f_params(1)

t_max = 50
dt = 0.02

num_steps = int(t_max / dt) + 2
x = np.zeros(num_steps - 1)
y = np.zeros(num_steps - 1)
z = np.zeros(num_steps - 1)

x[0], y[0], z[0] = v0

idx = 1
while r.successful() and r.t < t_max:
    r.integrate(r.t + dt)
    x[idx], y[idx], z[idx] = r.y[0:3]
    idx += 1

# Crie um array equivalente ao time_steps
time_steps = np.arange(0.0, t_max, dt)

dados = np.hstack((x[:-2].reshape(-1,1),y[:-2].reshape(-1,1),z[:-2].reshape(-1,1),x[1:-1].reshape(-1,1),y[1:-1].reshape(-1,1),z[1:-1].reshape(-1,1)))

# Determina o tamanho dos conjuntos de treino e teste
tam_treino = int(len(dados) * 0.8)
tam_teste = len(dados) - tam_treino

# Divide os dados em conjuntos de treino e teste
treino = dados[:tam_treino]
teste = dados[tam_treino:]

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x_train = scaler.fit_transform(treino)
x_teste = scaler.fit_transform(teste)

radius = 0.6
sparsity = 0.01
input_dim = 6
reservoir_size = 1000
n_steps_prerun = 10
regularization = 1e-2
sequence = x_train

# Inicialize aleatoriamente os pesos de rede
weights_hidden = sparse.random(reservoir_size, reservoir_size, density=sparsity)
eigenvalues, _ = sparse.linalg.eigs(weights_hidden)
weights_hidden = weights_hidden / np.max(np.abs(eigenvalues)) * radius

weights_input = np.zeros((reservoir_size, input_dim))
q = int(reservoir_size / input_dim)

for i in range(0, input_dim):
    weights_input[i * q:(i + 1) * q, i] = 2 * np.random.rand(q) - 1

weights_output = np.zeros((input_dim, reservoir_size))

# Incorporar a sequência no estado oculto da rede

def initialize_hidden(reservoir_size, n_steps_prerun, sequence):
    hidden = np.zeros((reservoir_size, 1))
    for t in range(n_steps_prerun):
        input = sequence[t].reshape(-1, 1)
        hidden = np.tanh(weights_hidden @ hidden + weights_input @ input)
    return hidden

def augment_hidden(hidden):
    h_aug = hidden.copy()
    h_aug[::2] = pow(h_aug[::2], 2.0)
    return h_aug

hidden = initialize_hidden(reservoir_size, n_steps_prerun, sequence)
hidden_states = []
targets = []

for t in range(n_steps_prerun, len(sequence) - 1):
    input = np.reshape(sequence[t], (-1, 1))
    target = np.reshape(sequence[t + 1], (-1, 1))
    hidden = np.tanh(weights_hidden @ hidden + weights_input @ input)
    hidden = augment_hidden(hidden)
    hidden_states.append(hidden)
    targets.append(target)

targets = np.squeeze(np.array(targets))
hidden_states = np.squeeze(np.array(hidden_states))

# Ridge Regression para obter os pesos da camada de saída linear
# Ridge Regression é uma extensão da regressão linear que adiciona uma penalidade de regularização à função de perda durante o treinamento.

weights_output = (np.linalg.inv(hidden_states.T@hidden_states + regularization * np.eye(reservoir_size)) @ hidden_states.T@targets).T

def predict(sequence, n_steps_predict):
    hidden = initialize_hidden(reservoir_size, n_steps_prerun, sequence)
    input = sequence[n_steps_prerun].reshape((-1, 1))
    outputs = []

    for t in range(n_steps_prerun, n_steps_prerun + n_steps_predict):
        hidden = np.tanh(weights_hidden @ hidden + weights_input @ input)
        hidden = augment_hidden(hidden)
        output = weights_output @ hidden
        input = output
        outputs.append(output)
    return np.array(outputs)

x_sim = predict(sequence, 4000)

plt.figure(figsize=(6, 4))
plt.plot(x_train[:4000, 0], x_train[:4000, 2], label = "Verdade Fundamental")
plt.plot(x_sim[:, 0], x_sim[:, 2],'--', label = "'Reservoir computing' Estimativa")
plt.plot(x_train[0, 0], x_train[0, 2], "ko", label = "Condição Inicial", markersize=8)

plt.legend()
plt.show()

x_sim = x_sim[:len(x_train)]

def plot_dimension(dim, name):
    fig = plt.figure(figsize=(9,2))
    ax = fig.gca()
    ax.plot(time_steps[:len(x_train[:, dim])], x_train[:, dim])
    ax.plot(time_steps[:len(x_sim[:, dim])], x_sim[:, dim], "--")
    plt.xlabel("time")
    plt.ylabel(name)

plot_dimension(0, 'x')
plot_dimension(1, 'y')
plot_dimension(2, 'z')

"""# Metricas de avaliação"""

import nolds

h = nolds.dfa(x_train)
h

import nolds

h = nolds.dfa(x_teste)
h

import nolds

h = nolds.dfa(x_sim)
h